{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ “cells”: \\[ { “cell_type”: “markdown”, “metadata”: {}, “source”: \\[\n",
    "“\\# Detecting Fake News using Support Vector Machines (SVM)”, “\\###\n",
    "Step-by-Step Implementation in Google Colab with Multilingual Support\n",
    "(English & Chinese)” \\] }, { “cell_type”: “markdown”, “metadata”: {},\n",
    "“source”: \\[ “\\## Step 1: Install & Import Necessary Libraries”, “We\n",
    "first import all the required Python libraries. NLTK is used for natural\n",
    "language processing, sklearn for machine learning, and pandas for data\n",
    "manipulation. We also use googletrans for translating Chinese text into\n",
    "English.” \\] }, { “cell_type”: “code”, “execution_count”: null,\n",
    "“metadata”: {}, “outputs”: \\[\\], “source”: \\[ “\\# Install & Import\n",
    "Necessary Libraries”, “!pip install googletrans==4.0.0-rc1”, “import\n",
    "pandas as pd \\# Data handling”, “import numpy as np \\# Numerical\n",
    "operations”, “import re \\# Regular expressions for text cleaning”,\n",
    "“import string \\# String operations”, “import nltk \\# Natural Language\n",
    "Processing”, “from nltk.corpus import stopwords \\# List of stopwords”,\n",
    "“from nltk.tokenize import word_tokenize \\# Tokenization”, “from\n",
    "nltk.stem import WordNetLemmatizer \\# Lemmatization”, “from googletrans\n",
    "import Translator \\# Translation from Chinese to English”, “from\n",
    "sklearn.feature_extraction.text import TfidfVectorizer \\# Convert text\n",
    "to numerical representation”, “from sklearn.model_selection import\n",
    "train_test_split \\# Splitting data”, “from sklearn.svm import SVC \\#\n",
    "Support Vector Machine model”, “from sklearn.metrics import\n",
    "accuracy_score, classification_report \\# Model evaluation”, “”, “\\#\n",
    "Download necessary resources for NLTK”, “nltk.download(‘stopwords’)”,\n",
    "“nltk.download(‘punkt’)”, “nltk.download(‘wordnet’)” \\] }, {\n",
    "“cell_type”: “markdown”, “metadata”: {}, “source”: \\[ “\\## Step 2: Load\n",
    "and Merge Datasets”, “We will load and merge two datasets: Weibo21\n",
    "(train, test, val) and the Kaggle Fake News dataset (True.csv,\n",
    "Fake.csv). Chinese text will be translated to English.” \\] }, {\n",
    "“cell_type”: “code”, “execution_count”: null, “metadata”: {}, “outputs”:\n",
    "\\[\\], “source”: \\[ “\\# Load and Merge Datasets”, “df_train =\n",
    "pd.read_csv(‘train.csv’)”, “df_test = pd.read_csv(‘test.csv’)”, “df_val\n",
    "= pd.read_csv(‘val.csv’)”, “df_fake = pd.read_csv(‘Fake.csv’)”, “df_real\n",
    "= pd.read_csv(‘True.csv’)”, “df_fake\\[‘label’\\] = 1”,\n",
    "“df_real\\[‘label’\\] = 0”, “df_train\\[‘label’\\] =\n",
    "df_train\\[‘label’\\].map({‘fake’: 1, ‘real’: 0})”, “df_test\\[‘label’\\] =\n",
    "df_test\\[‘label’\\].map({‘fake’: 1, ‘real’: 0})”, “df_val\\[‘label’\\] =\n",
    "df_val\\[‘label’\\].map({‘fake’: 1, ‘real’: 0})”, “df_kaggle =\n",
    "pd.concat(\\[df_fake, df_real\\])”, “df_weibo = pd.concat(\\[df_train,\n",
    "df_test, df_val\\])”, “”, “\\# Translate Chinese text to English”,\n",
    "“translator = Translator()”, “df_weibo\\[‘content’\\] =\n",
    "df_weibo\\[‘content’\\].apply(lambda x: translator.translate(x,\n",
    "src=‘zh-cn’, dest=‘en’).text)”, “”, “\\# Merge both datasets”,\n",
    "“df_combined = pd.concat(\\[df_kaggle\\[\\[‘title’, ‘text’, ‘label’\\]\\],\n",
    "df_weibo\\[\\[‘content’, ‘label’\\]\\].rename(columns={‘content’:\n",
    "‘text’})\\])”, “df_combined =\n",
    "df_combined.sample(frac=1).reset_index(drop=True) \\# Shuffle data”,\n",
    "“df_combined.head()” \\] }, { “cell_type”: “markdown”, “metadata”: {},\n",
    "“source”: \\[ “\\## Step 3: Data Preprocessing”, “We clean the text by\n",
    "converting it to lowercase, removing punctuation, and lemmatizing words\n",
    "to their base form.” \\] }, { “cell_type”: “code”, “execution_count”:\n",
    "null, “metadata”: {}, “outputs”: \\[\\], “source”: \\[ “\\# Data\n",
    "Preprocessing”, “lemmatizer = WordNetLemmatizer()”, “stop_words =\n",
    "set(stopwords.words(‘english’))”, “def clean_text(text):”, ” text =\n",
    "text.lower()“,” text = re.sub(r’d+‘,’‘, text)“,” text =\n",
    "re.sub(r’\\[^ws\\]‘,’‘, text)“,” text =’\n",
    "’.join(\\[lemmatizer.lemmatize(word) for word in word_tokenize(text) if\n",
    "word not in stop_words\\])“,” return\n",
    "text“,”df_combined\\[‘processed_text’\\] =\n",
    "df_combined\\[‘text’\\].apply(clean_text)“,”df_combined\\[\\[‘text’,\n",
    "‘processed_text’\\]\\].head()” \\] }, { “cell_type”: “markdown”,\n",
    "“metadata”: {}, “source”: \\[ “\\## Step 4: Feature Extraction using\n",
    "TF-IDF”, “Convert the cleaned text into numerical representation using\n",
    "TF-IDF vectorization.” \\] }, { “cell_type”: “code”, “execution_count”:\n",
    "null, “metadata”: {}, “outputs”: \\[\\], “source”: \\[ “\\# Feature\n",
    "Extraction”, “vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "max_features=5000)”, “X =\n",
    "vectorizer.fit_transform(df_combined\\[‘processed_text’\\])”, “y =\n",
    "df_combined\\[‘label’\\]” \\] }, { “cell_type”: “markdown”, “metadata”: {},\n",
    "“source”: \\[ “\\## Step 5: Train and Evaluate SVM Model”, “Train an SVM\n",
    "classifier and evaluate its performance.” \\] }, { “cell_type”: “code”,\n",
    "“execution_count”: null, “metadata”: {}, “outputs”: \\[\\], “source”: \\[\n",
    "“\\# Train and Evaluate Model”, “X_train, X_test, y_train, y_test =\n",
    "train_test_split(X, y, test_size=0.2, random_state=42)”, “svm_model =\n",
    "SVC(kernel=‘linear’, C=1)”, “svm_model.fit(X_train, y_train)”, “y_pred =\n",
    "svm_model.predict(X_test)”, “print(‘Accuracy:’, accuracy_score(y_test,\n",
    "y_pred))”, “print(‘Classification Report:’,\n",
    "classification_report(y_test, y_pred))” \\] } \\], “metadata”: {},\n",
    "“nbformat”: 4, “nbformat_minor”: 4 }"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
